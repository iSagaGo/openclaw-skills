#!/usr/bin/env python3
"""
é“¾ä¸Šé¡¹ç›®ç›‘æ§ - 48å°æ—¶å›æµ‹
ç”¨ä¸ gmgn_monitor.py ç›¸åŒçš„ä¸‰ä¸ªæ•°æ®æºå’Œè¿‡æ»¤é€»è¾‘ï¼Œ
æ‹‰å–è¿‡å»48å°æ—¶å†…çš„é¡¹ç›®å¹¶è¾“å‡ºç»“æœã€‚
"""

import json
import os
import time
import requests
from datetime import datetime

CHAIN = "base"
MIN_LIQUIDITY = 5000
MIN_HOLDERS = 20
MAX_AGE_HOURS = 48  # å›æµ‹48å°æ—¶

AI_MINING_KEYWORDS = [
    "mine", "miner", "mining", "bot", "agent", "ai",
    "earn", "farm", "stake", "proof", "compute",
    "gpu", "hash", "reward", "epoch", "node",
    "botcoin", "agentcoin", "aibot", "automine"
]

DEXSCREENER_KEYWORDS = [
    "botcoin", "mining", "miner", "ai agent", "bot coin",
    "agent coin", "compute", "gpu", "hash", "proof",
    "node", "earn", "farm", "stake", "reward",
]

EXCLUDED_SYMBOLS = [
    "cbBTC", "WETH", "USDC", "USDT", "DAI", "WBTC", "ETH",
    "USDbC", "AERO", "DEGEN", "BRETT", "TOSHI",
]
EXCLUDED_SYMBOLS_LOWER = [s.lower() for s in EXCLUDED_SYMBOLS]

GMGN_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Referer": "https://gmgn.ai/?chain=base",
    "Accept": "application/json",
    "Origin": "https://gmgn.ai"
}

DEXSCREENER_HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Accept": "application/json",
}

NOW = int(time.time())
CUTOFF = NOW - 48 * 3600


def log(msg):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}", flush=True)


def is_ai_mining(text_parts):
    text = ' '.join(t.lower() for t in text_parts if t)
    matches = [kw for kw in AI_MINING_KEYWORDS if kw in text]
    return len(matches) > 0, matches


# === GMGN token è¯¦æƒ… API ===
# api/v1/token_info å¯è·å–å•ä¸ª token çš„ holder_count ç­‰è¯¦æƒ…
def fetch_gmgn_token_detail(address):
    """ä» GMGN api/v1/token_info è·å–å•ä¸ª token è¯¦æƒ…"""
    try:
        url = f"https://gmgn.ai/api/v1/token_info/{CHAIN}/{address}"
        resp = requests.get(url, headers=GMGN_HEADERS, timeout=10)
        data = resp.json()
        if data.get('code') == 0:
            return data.get('data', {})
    except:
        pass
    return {}


# === Honeypot.is API ===
# é€šè¿‡æ¨¡æ‹Ÿé“¾ä¸Šäº¤æ˜“æ£€æµ‹çœŸå®ä¹°å–ç¨ç‡å’Œè²”è²…çŠ¶æ€
def fetch_honeypot_check(address):
    """ä» honeypot.is è·å–çœŸå®ç¨ç‡å’Œè²”è²…æ£€æµ‹ç»“æœ"""
    try:
        url = f"https://api.honeypot.is/v2/IsHoneypot?address={address}&chainId=8453"
        resp = requests.get(url, timeout=10)
        data = resp.json()
        result = {}
        if data.get('honeypotResult'):
            result['is_honeypot'] = 1 if data['honeypotResult'].get('isHoneypot') else 0
            result['honeypot_reason'] = data['honeypotResult'].get('honeypotReason', '')
        if data.get('simulationResult'):
            result['buy_tax'] = float(data['simulationResult'].get('buyTax') or 0)
            result['sell_tax'] = float(data['simulationResult'].get('sellTax') or 0)
        if data.get('summary'):
            result['risk_level'] = int(data['summary'].get('riskLevel') or 0)
        return result
    except:
        pass
    return {}


# === æ•°æ®æº 1: GMGN rank ===
def fetch_gmgn_rank():
    all_tokens = []
    for timeframe in ['1h', '6h', '24h']:
        try:
            url = f"https://gmgn.ai/defi/quotation/v1/rank/{CHAIN}/swaps/{timeframe}"
            params = {"limit": 100, "orderby": "open_timestamp", "direction": "desc", "tag": "graduated"}
            resp = requests.get(url, params=params, headers=GMGN_HEADERS, timeout=15)
            data = resp.json()
            if data.get('code') == 0:
                tokens = data['data']['rank']
                log(f"[GMGN-rank/{timeframe}] {len(tokens)} ä¸ªé¡¹ç›®")
                all_tokens.extend(tokens)
            time.sleep(0.5)
        except Exception as e:
            log(f"[GMGN-rank/{timeframe}] error: {e}")
    return all_tokens


def parse_gmgn_rank(t):
    open_ts = t.get('open_timestamp') or 0
    age_hours = (NOW - open_ts) / 3600
    return {
        'address': (t.get('address') or '').lower(),
        'symbol': t.get('symbol', '?'),
        'price': t.get('price', 0),
        'market_cap': float(t.get('market_cap') or 0),
        'liquidity': float(t.get('liquidity') or 0),
        'volume_1h': float(t.get('volume') or 0),
        'swaps': int(t.get('swaps') or 0),
        'buys': int(t.get('buys') or 0),
        'sells': int(t.get('sells') or 0),
        'holders': int(t.get('holder_count') or 0),
        'price_change_1h': t.get('price_change_percent1h', 0),
        'age_hours': round(age_hours, 1),
        'open_timestamp': open_ts,
        'twitter': t.get('twitter_username') or '',
        'website': t.get('website') or '',
        'telegram': t.get('telegram') or '',
        'source': 'gmgn_rank',
        'is_honeypot': int(t.get('is_honeypot') or 0),
        'buy_tax': float(t.get('buy_tax') or 0),
        'sell_tax': float(t.get('sell_tax') or 0),
        'renounced': int(t.get('renounced') or 0),
        'is_open_source': int(t.get('is_open_source') or 0),
        'rug_ratio': float(t.get('rug_ratio') or 0),
    }


# === æ•°æ®æº 2: GMGN new_pairs ===
def fetch_gmgn_pairs():
    try:
        url = f"https://gmgn.ai/defi/quotation/v1/pairs/{CHAIN}/new_pairs"
        params = {"limit": 100, "orderby": "open_timestamp", "direction": "desc"}
        resp = requests.get(url, params=params, headers=GMGN_HEADERS, timeout=15)
        data = resp.json()
        if data.get('code') == 0:
            pairs = data['data'].get('pairs', [])
            log(f"[GMGN-pairs] {len(pairs)} ä¸ªæ–°äº¤æ˜“å¯¹")
            return pairs
        log(f"[GMGN-pairs] API error: {data.get('msg')}")
    except Exception as e:
        log(f"[GMGN-pairs] error: {e}")
    return []


def parse_gmgn_pair(p):
    bti = p.get('base_token_info', {})
    open_ts = p.get('open_timestamp') or 0
    age_hours = (NOW - open_ts) / 3600 if open_ts else 0
    social = bti.get('social_links', {}) or {}
    return {
        'address': (bti.get('address') or '').lower(),
        'symbol': bti.get('symbol', '?'),
        'price': bti.get('price', 0),
        'market_cap': float(bti.get('market_cap') or 0),
        'liquidity': float(bti.get('liquidity') or 0),
        'volume_1h': float(bti.get('volume') or 0),
        'swaps': int(bti.get('swaps') or 0),
        'buys': int(bti.get('buys') or 0),
        'sells': int(bti.get('sells') or 0),
        'holders': int(bti.get('holder_count') or 0),
        'price_change_1h': bti.get('price_change_percent1h', 0),
        'age_hours': round(age_hours, 1),
        'open_timestamp': open_ts,
        'twitter': social.get('twitter_username') or '',
        'website': social.get('website') or '',
        'telegram': social.get('telegram') or '',
        'source': 'gmgn_pairs',
        'is_honeypot': int(bti.get('is_honeypot') or 0),
        'buy_tax': float(bti.get('buy_tax') or 0),
        'sell_tax': float(bti.get('sell_tax') or 0),
        'renounced': int(bti.get('renounced') or 0),
        'is_open_source': int(bti.get('is_open_source') or 0),
        'rug_ratio': float(bti.get('rug_ratio') or 0),
    }


# === æ•°æ®æº 3: DexScreener ===
def fetch_dexscreener():
    all_tokens = {}
    for kw in DEXSCREENER_KEYWORDS:
        try:
            resp = requests.get(
                f'https://api.dexscreener.com/latest/dex/search?q={kw}',
                headers=DEXSCREENER_HEADERS, timeout=15
            )
            if resp.status_code != 200:
                continue
            d = resp.json()
            pairs = [p for p in d.get('pairs', []) if p.get('chainId') == 'base']
            for p in pairs:
                addr = (p.get('baseToken', {}).get('address') or '').lower()
                if addr and addr not in all_tokens:
                    all_tokens[addr] = p
            time.sleep(0.3)
        except Exception as e:
            log(f"[DexScreener] '{kw}' error: {e}")
    log(f"[DexScreener] å…± {len(all_tokens)} ä¸ª Base é“¾é¡¹ç›®")
    return list(all_tokens.values())


def parse_dexscreener(p):
    bt = p.get('baseToken', {})
    now_ms = time.time() * 1000
    created = p.get('pairCreatedAt') or 0
    age_hours = (now_ms - created) / 3600000 if created else 0
    txns_1h = p.get('txns', {}).get('h1', {})
    info = p.get('info', {})
    socials = info.get('socials', [])
    websites = info.get('websites', [])
    twitter, website, telegram = '', '', ''
    for s in socials:
        if s.get('type') == 'twitter':
            url = s.get('url', '')
            twitter = url.split('/')[-1] if '/' in url else url
        elif s.get('type') == 'telegram':
            telegram = s.get('url', '')
    if websites:
        website = websites[0].get('url', '')
    return {
        'address': (bt.get('address') or '').lower(),
        'symbol': bt.get('symbol', '?'),
        'price': float(p.get('priceUsd') or 0),
        'market_cap': float(p.get('marketCap') or 0),
        'liquidity': float((p.get('liquidity') or {}).get('usd') or 0),
        'volume_1h': float((p.get('volume') or {}).get('h1') or 0),
        'swaps': int(txns_1h.get('buys', 0)) + int(txns_1h.get('sells', 0)),
        'buys': int(txns_1h.get('buys', 0)),
        'sells': int(txns_1h.get('sells', 0)),
        'holders': 0,
        'price_change_1h': float((p.get('priceChange') or {}).get('h1') or 0),
        'age_hours': round(age_hours, 1),
        'open_timestamp': int(created / 1000) if created else 0,
        'twitter': twitter,
        'website': website,
        'telegram': telegram,
        'source': 'dexscreener',
        'is_honeypot': None,
        'buy_tax': None,
        'sell_tax': None,
        'renounced': None,
        'is_open_source': None,
        'rug_ratio': None,
    }


def main():
    log("=" * 60)
    log("é“¾ä¸Šé¡¹ç›®ç›‘æ§ - 48å°æ—¶å›æµ‹")
    log(f"å›æµ‹èŒƒå›´: {datetime.fromtimestamp(CUTOFF).strftime('%m-%d %H:%M')} ~ {datetime.fromtimestamp(NOW).strftime('%m-%d %H:%M')}")
    log(f"è¿‡æ»¤: æµåŠ¨æ€§>=${MIN_LIQUIDITY} æŒæœ‰äºº>={MIN_HOLDERS} å¹´é¾„<={MAX_AGE_HOURS}h")
    log("=" * 60)

    all_parsed = []

    # æº1
    for t in fetch_gmgn_rank():
        all_parsed.append(parse_gmgn_rank(t))
    # æº2
    for p in fetch_gmgn_pairs():
        all_parsed.append(parse_gmgn_pair(p))
    # æº3
    for p in fetch_dexscreener():
        all_parsed.append(parse_dexscreener(p))

    log(f"\nåŸå§‹æ•°æ®: {len(all_parsed)} æ¡")

    # å»é‡ï¼ˆä¼˜å…ˆçº§é«˜çš„è¦†ç›–ä½çš„ï¼‰ï¼ŒåŒæ—¶ä¿ç•™æ‰€æœ‰æºæ•°æ®ç”¨äºäº¤å‰è¡¥å……
    merged = {}
    all_by_addr = {}  # ä¿ç•™æ‰€æœ‰æºçš„æ•°æ®
    priority = {'gmgn_rank': 3, 'gmgn_pairs': 2, 'dexscreener': 1}
    for t in all_parsed:
        addr = t.get('address', '')
        if not addr:
            continue
        if addr not in all_by_addr:
            all_by_addr[addr] = []
        all_by_addr[addr].append(t)
        existing = merged.get(addr)
        if not existing or priority.get(t['source'], 0) > priority.get(existing['source'], 0):
            merged[addr] = t

    # äº¤å‰è¡¥å……ï¼šç”¨ GMGN æ•°æ®è¡¥å…… DexScreener ç¼ºå¤±çš„å­—æ®µ
    for addr, main in merged.items():
        others = all_by_addr.get(addr, [])
        for other in others:
            if other['source'] == main['source']:
                continue
            # è¡¥å……æŒæœ‰äººæ•°æ®
            if main['holders'] == 0 and other['holders'] > 0:
                main['holders'] = other['holders']
            # è¡¥å……ç½‘ç«™/æ¨ç‰¹/ç”µæŠ¥
            if not main['website'] and other.get('website'):
                main['website'] = other['website']
            if not main['twitter'] and other.get('twitter'):
                main['twitter'] = other['twitter']
            if not main['telegram'] and other.get('telegram'):
                main['telegram'] = other['telegram']
            # è¡¥å…… ageï¼ˆDexScreener age=0 æ—¶ç”¨ GMGN çš„ï¼‰
            if main['age_hours'] == 0 and other['age_hours'] > 0:
                main['age_hours'] = other['age_hours']
                main['open_timestamp'] = other['open_timestamp']
            # è¡¥å……å®‰å…¨å­—æ®µï¼ˆDexScreener æ²¡æœ‰è¿™äº›æ•°æ®ï¼‰
            for field in ['is_honeypot', 'buy_tax', 'sell_tax', 'renounced', 'is_open_source', 'rug_ratio']:
                if main.get(field) is None and other.get(field) is not None:
                    main[field] = other[field]

    log(f"å»é‡å: {len(merged)} ä¸ªå”¯ä¸€é¡¹ç›®")

    # 48å°æ—¶è¿‡æ»¤
    in_range = {k: v for k, v in merged.items() if v['age_hours'] <= MAX_AGE_HOURS}
    log(f"48å°æ—¶å†…: {len(in_range)} ä¸ª")

    # è´¨é‡è¿‡æ»¤
    quality = {}
    for k, v in in_range.items():
        # æ’é™¤ä¸»æµå¸
        if v['symbol'].lower() in EXCLUDED_SYMBOLS_LOWER:
            continue
        if v['liquidity'] < MIN_LIQUIDITY:
            continue
        if v['holders'] > 0 and v['holders'] < MIN_HOLDERS:
            continue
        quality[k] = v

    # å¯¹æŒæœ‰äººä¸º0çš„é¡¹ç›®ï¼Œä» GMGN token_info API è¡¥æŸ¥
    missing_holders = [addr for addr, v in quality.items() if v['holders'] == 0]
    if missing_holders:
        log(f"è¡¥æŸ¥ {len(missing_holders)} ä¸ªç¼ºå¤±æŒæœ‰äººæ•°æ®çš„é¡¹ç›®...")
        for addr in missing_holders:
            info = fetch_gmgn_token_detail(addr)
            if info:
                holders = int(info.get('holder_count') or 0)
                if holders > 0:
                    quality[addr]['holders'] = holders
                    log(f"  {quality[addr]['symbol']}: è¡¥å……æŒæœ‰äºº {holders}")
                # åŒæ—¶è¡¥å……å…¶ä»–ç¼ºå¤±å­—æ®µ
                if not quality[addr].get('website') and info.get('website'):
                    quality[addr]['website'] = info['website']
                if not quality[addr].get('twitter') and info.get('twitter_username'):
                    quality[addr]['twitter'] = info['twitter_username']
                if not quality[addr].get('telegram') and info.get('telegram'):
                    quality[addr]['telegram'] = info['telegram']
                # è¡¥å…… age
                if quality[addr]['age_hours'] == 0 and info.get('open_timestamp'):
                    ots = int(info['open_timestamp'])
                    quality[addr]['age_hours'] = round((NOW - ots) / 3600, 1)
                    quality[addr]['open_timestamp'] = ots
            time.sleep(0.3)
        # è¡¥æŸ¥åé‡æ–°è¿‡æ»¤æŒæœ‰äººä¸è¶³çš„å’Œè¶…é¾„çš„
        to_remove = [k for k, v in quality.items()
                     if (0 < v['holders'] < MIN_HOLDERS) or v['age_hours'] > MAX_AGE_HOURS]
        for k in to_remove:
            del quality[k]

    # æµåŠ¨æ€§äºŒæ¬¡éªŒè¯ + Honeypot.is çœŸå®ç¨ç‡æ£€æµ‹
    log(f"æµåŠ¨æ€§äºŒæ¬¡éªŒè¯ + è²”è²…æ£€æµ‹...")
    to_remove = []
    for addr, v in quality.items():
        # 1. GMGN æµåŠ¨æ€§éªŒè¯
        info = fetch_gmgn_token_detail(addr)
        if info:
            real_liq = float(info.get('liquidity') or 0)
            if real_liq < MIN_LIQUIDITY and v['liquidity'] >= MIN_LIQUIDITY:
                log(f"  âŒ {v['symbol']}: å®é™…æµåŠ¨æ€§ ${real_liq:,.0f}ï¼ˆåˆ—è¡¨æ˜¾ç¤º ${v['liquidity']:,.0f}ï¼‰ï¼Œå·²å‰”é™¤")
                to_remove.append(addr)
                continue
            elif real_liq > 0 and v['liquidity'] > 0:
                v['liquidity'] = real_liq
        # 2. Honeypot.is çœŸå®ç¨ç‡æ£€æµ‹
        hp = fetch_honeypot_check(addr)
        if hp:
            if hp.get('is_honeypot') == 1:
                v['is_honeypot'] = 1
                v['honeypot_reason'] = hp.get('honeypot_reason', '')
                log(f"  ğŸš« {v['symbol']}: è²”è²…ç›˜ï¼{hp.get('honeypot_reason','')}")
            if hp.get('buy_tax') is not None:
                v['buy_tax'] = hp['buy_tax']
            if hp.get('sell_tax') is not None:
                v['sell_tax'] = hp['sell_tax']
            if hp.get('sell_tax', 0) >= 50:
                log(f"  âš ï¸ {v['symbol']}: å–å‡ºç¨ {hp['sell_tax']}%")
        time.sleep(0.2)
    for k in to_remove:
        del quality[k]

    log(f"è´¨é‡è¿‡æ»¤å: {len(quality)} ä¸ª")

    # AI æŒ–çŸ¿æ ‡è®°
    for t in quality.values():
        is_ai, kws = is_ai_mining([t['symbol'], t['website'], t['twitter']])
        t['is_ai_mining'] = is_ai
        t['ai_keywords'] = kws

    # æ’åº
    results = sorted(quality.values(), key=lambda x: (not x['is_ai_mining'], -x['open_timestamp']))

    ai_count = sum(1 for r in results if r['is_ai_mining'])
    log(f"\n{'=' * 60}")
    log(f"å›æµ‹ç»“æœ: {len(results)} ä¸ªé¡¹ç›® (AIæŒ–çŸ¿: {ai_count})")
    log(f"{'=' * 60}")

    # æ•°æ®æºç»Ÿè®¡
    src_count = {}
    for r in results:
        src = r['source']
        src_count[src] = src_count.get(src, 0) + 1
    for src, cnt in sorted(src_count.items()):
        log(f"  æ•°æ®æº {src}: {cnt} ä¸ª")

    # æ•°æ®å¼‚å¸¸æ£€æµ‹
    def get_warnings(p):
        warns = []
        mc = p['market_cap']
        liq = p['liquidity']
        if liq > 0 and mc / liq > 1000:
            warns.append(f"MC/Liqæ¯”={mc/liq:.0f}xï¼Œç–‘ä¼¼å‡å¸‚å€¼")
        elif liq > 0 and mc / liq > 100:
            warns.append(f"MC/Liqæ¯”={mc/liq:.0f}xï¼Œå¸‚å€¼åé«˜")
        if p['age_hours'] == 0 and p['source'] == 'dexscreener':
            warns.append("age=0hï¼Œå¯èƒ½æ˜¯æ–°pairéæ–°å¸")
        if p['holders'] == 0 and p['source'] == 'dexscreener':
            warns.append("æŒæœ‰äººæ•°æ®ç¼ºå¤±")
        # è²”è²…ç›˜æ£€æµ‹
        if p.get('is_honeypot') == 1:
            warns.append("ğŸš« è²”è²…ç›˜ï¼ˆHoneypotï¼‰ï¼åªèƒ½ä¹°ä¸èƒ½å–")
        # ä¹°å–ç¨æ£€æµ‹
        buy_tax = p.get('buy_tax')
        sell_tax = p.get('sell_tax')
        if buy_tax is not None and float(buy_tax) > 5:
            warns.append(f"ä¹°å…¥ç¨ {float(buy_tax):.1f}%")
        if sell_tax is not None and float(sell_tax) > 5:
            warns.append(f"å–å‡ºç¨ {float(sell_tax):.1f}%")
        if sell_tax is not None and float(sell_tax) > 30:
            warns.append("ğŸš« å–å‡ºç¨è¿‡é«˜ï¼Œç–‘ä¼¼è²”è²…")
        # Rug é£é™©
        rug = p.get('rug_ratio')
        if rug is not None and float(rug) > 0.5:
            warns.append(f"â›” Rugé£é™© {float(rug)*100:.0f}%")
        elif rug is not None and float(rug) > 0.2:
            warns.append(f"Rugé£é™© {float(rug)*100:.0f}%")
        return warns

    # å®‰å…¨æ ‡ç­¾ç”Ÿæˆ
    def get_security_tags(p):
        tags = []
        if p.get('renounced') == 1:
            tags.append("âœ…å¼ƒæƒ")
        elif p.get('renounced') == 0 and p.get('renounced') is not None:
            tags.append("âŒæœªå¼ƒæƒ")
        if p.get('is_open_source') == 1:
            tags.append("âœ…å¼€æº")
        elif p.get('is_open_source') == 0 and p.get('is_open_source') is not None:
            tags.append("âŒæœªå¼€æº")
        if p.get('is_honeypot') == 1:
            tags.append("ğŸš«è²”è²…")
        elif p.get('is_honeypot') == 0 and p.get('is_honeypot') is not None:
            tags.append("âœ…éè²”è²…")
        buy_tax = p.get('buy_tax')
        sell_tax = p.get('sell_tax')
        if buy_tax is not None and sell_tax is not None:
            tags.append(f"ç¨:{float(buy_tax):.1f}%/{float(sell_tax):.1f}%")
        return tags

    # é‡ç‚¹é¡¹ç›®æ£€æµ‹ï¼ˆæœ‰ç½‘é¡µ/ç¤¾äº¤èµ„æ–™ï¼‰
    def is_featured(p):
        return bool(p.get('website') or p.get('twitter') or p.get('telegram'))

    # åŒåæ£€æµ‹ï¼šå½“æ‰¹ç»“æœ + å†å² notified_full åˆå¹¶ç»Ÿè®¡
    from collections import Counter
    _symbol_counts = Counter(r['symbol'] for r in results)
    # åˆå¹¶å†å²æ•°æ®ä¸­çš„ symbol
    try:
        import json as _json
        _state_file = "/tmp/gmgn_monitor_state.json"
        if os.path.exists(_state_file):
            with open(_state_file) as _sf:
                _hist = _json.load(_sf).get('notified_full', {})
            for _addr, _hp in _hist.items():
                _sym = _hp.get('symbol', '')
                if _sym:
                    _symbol_counts[_sym] += 1
            # å»æ‰å½“æ‰¹å·²ç»Ÿè®¡çš„é‡å¤ï¼ˆå†å²é‡Œä¹Ÿæœ‰å½“æ‰¹é¡¹ç›®ï¼‰
            for r in results:
                if r['address'] in _hist:
                    _symbol_counts[r['symbol']] -= 1
    except Exception:
        pass

    def _display_name(p):
        """åŒåé¡¹ç›®åœ¨ symbol ååŠ åˆçº¦åœ°å€å‰6ä½+åŒåæ•°é‡"""
        sym = p['symbol']
        cnt = _symbol_counts.get(sym, 1)
        if cnt > 1:
            return f"{sym} ({p['address'][:6]}) [åŒåÃ—{cnt}]"
        return sym

    def format_project(i, p, show_keywords=False):
        lines = []
        warns = get_warnings(p)
        featured = is_featured(p)
        # æ ‡é¢˜è¡Œ
        prefix = ""
        if featured and warns:
            prefix = "â­âš ï¸ "
        elif featured:
            prefix = "â­ "
        elif warns:
            prefix = "âš ï¸ "
        suffix = ""
        if featured:
            suffix += " â€” æœ‰ç½‘é¡µèµ„æ–™"
        lines.append(f"  {prefix}#{i} {_display_name(p)}{suffix}")
        lines.append(f"     åˆçº¦: {p['address']}")
        lines.append(f"     MC: ${p['market_cap']:,.0f} | æµåŠ¨æ€§: ${p['liquidity']:,.0f} | æŒæœ‰äºº: {p['holders']}")
        try:
            chg_val = float(p['price_change_1h']) if p['price_change_1h'] else 0
        except (ValueError, TypeError):
            chg_val = 0
        chg_str = f"+{chg_val:.1f}%" if chg_val > 0 else f"{chg_val:.1f}%"
        lines.append(f"     å¹´é¾„: {p['age_hours']}h | 1h: {chg_str} | æ¥æº: {p['source']}")
        # å®‰å…¨æ ‡ç­¾
        sec_tags = get_security_tags(p)
        if sec_tags:
            lines.append(f"     ğŸ”’ {' | '.join(sec_tags)}")
        if show_keywords and p.get('ai_keywords'):
            lines.append(f"     å…³é”®è¯: {', '.join(p['ai_keywords'])}")
        if p.get('website'):
            lines.append(f"     ğŸŒ {p['website']}")
        if p.get('twitter'):
            lines.append(f"     ğŸ¦ @{p['twitter']}")
        if p.get('telegram'):
            lines.append(f"     ğŸ’¬ {p['telegram']}")
        lines.append(f"     ğŸ”— gmgn.ai/base/token/{p['address']}")
        if warns:
            for w in warns:
                lines.append(f"     âš ï¸ {w}")
        lines.append("")
        return lines

    # ç–‘ä¼¼å‡å¸‚å€¼åˆ¤æ–­
    def _is_fake_mc(p):
        liq = p.get('liquidity', 0)
        mc = p.get('market_cap', 0)
        return liq > 0 and mc / liq > 1000

    # æ–°é¡¹ç›®æ£€æµ‹ï¼šä¸åœ¨å†å² notified_full é‡Œçš„
    _hist_addrs = set(_hist.keys()) if '_hist' in dir() else set()
    try:
        if not _hist_addrs:
            import json as _json2
            _sf2 = "/tmp/gmgn_monitor_state.json"
            if os.path.exists(_sf2):
                with open(_sf2) as _f2:
                    _hist_addrs = set(_json2.load(_f2).get('notified_full', {}).keys())
    except Exception:
        pass

    def _is_new(p):
        return p['address'] not in _hist_addrs

    # å››åˆ†ç±»ï¼šæ–°é¡¹ç›®ã€AIæŒ–çŸ¿ã€å…¶ä»–ã€ç–‘ä¼¼å‡å¸‚å€¼
    new_projects = [r for r in results if _is_new(r) and not _is_fake_mc(r)]
    new_addrs = {r['address'] for r in new_projects}
    ai_projects = [r for r in results if r['is_ai_mining'] and not _is_fake_mc(r) and r['address'] not in new_addrs]
    normal = [r for r in results if not r['is_ai_mining'] and not _is_fake_mc(r) and r['address'] not in new_addrs]
    fake_mc = [r for r in results if _is_fake_mc(r)]

    if new_projects:
        log(f"\nğŸ†• æ–°é¡¹ç›® ({len(new_projects)}):")
        log("-" * 60)
        for i, p in enumerate(new_projects, 1):
            for line in format_project(i, p, show_keywords=p.get('is_ai_mining', False)):
                log(line)

    if ai_projects:
        log(f"\nğŸ¤– AI æŒ–çŸ¿é¡¹ç›® ({len(ai_projects)}):")
        log("-" * 60)
        for i, p in enumerate(ai_projects, 1):
            for line in format_project(i, p, show_keywords=True):
                log(line)

    if normal:
        log(f"\nğŸ“Š å…¶ä»–é¡¹ç›® ({len(normal)}):")
        log("-" * 60)
        for i, p in enumerate(normal, 1):
            for line in format_project(i, p):
                log(line)

    if fake_mc:
        log(f"\nâš ï¸ ç–‘ä¼¼å‡å¸‚å€¼ ({len(fake_mc)}):")
        log("-" * 60)
        for i, p in enumerate(fake_mc, 1):
            for line in format_project(i, p, show_keywords=p.get('is_ai_mining', False)):
                log(line)

    # ä¿å­˜å®Œæ•´ç»“æœ
    out_file = "/tmp/backtest_48h_results.json"
    with open(out_file, 'w') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    log(f"\nå®Œæ•´ç»“æœå·²ä¿å­˜: {out_file}")

    # ç”Ÿæˆæ ¼å¼åŒ–æŠ¥å‘Šæ–‡ä»¶ï¼ˆä¾› AI ç›´æ¥è½¬å‘ï¼Œçœ tokenï¼‰
    report_file = "/tmp/backtest_report.txt"
    _generate_report(results, new_projects, ai_projects, normal, fake_mc, _display_name, get_security_tags, get_warnings, report_file)
    log(f"æ ¼å¼åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


def _generate_report(results, new_projects, ai_projects, normal, fake_mc, display_name_fn, sec_tags_fn, warns_fn, out_path):
    """ç”Ÿæˆå¯ç›´æ¥å‘é€çš„æ ¼å¼åŒ–æŠ¥å‘Š"""
    from datetime import datetime, timedelta
    now = datetime.now()
    start = now - timedelta(hours=48)
    lines = []

    total = len(new_projects) + len(ai_projects) + len(normal) + len(fake_mc)
    lines.append(f"ğŸ“‹ 48å°æ—¶å›æµ‹ç»“æœï¼ˆ{start.strftime('%m-%d')} ~ {now.strftime('%m-%d')}ï¼‰")
    lines.append(f"")
    summary = f"{total} ä¸ªé¡¹ç›®"
    if new_projects:
        summary += f" | ğŸ†•æ–°: {len(new_projects)}"
    summary += f" | AIæŒ–çŸ¿: {len(ai_projects)} | å…¶ä»–: {len(normal)}"
    if fake_mc:
        summary += f" | ç–‘ä¼¼å‡å¸‚å€¼: {len(fake_mc)}"
    lines.append(summary)
    lines.append("")

    def _fmt_mc(val):
        if val >= 1_000_000_000:
            return f"${val/1_000_000_000:.2f}B"
        elif val >= 1_000_000:
            return f"${val/1_000_000:.2f}M"
        elif val >= 1_000:
            return f"${val/1_000:.0f}K"
        return f"${val:.0f}"

    def _fmt_project(i, p, show_kw=False):
        plines = []
        warns = warns_fn(p)
        has_web = bool(p.get('website') or p.get('twitter') or p.get('telegram'))
        prefix = ""
        if has_web and warns:
            prefix = "â­âš ï¸ "
        elif has_web:
            prefix = "â­ "
        elif warns:
            prefix = "âš ï¸ "
        suffix = ""
        if has_web:
            suffix = " â€” æœ‰ç½‘é¡µèµ„æ–™"
        plines.append(f"{prefix}#{i} {display_name_fn(p)}{suffix}")
        plines.append(f"åˆçº¦: {p['address']}")
        plines.append(f"MC: {_fmt_mc(p['market_cap'])} | æµåŠ¨æ€§: {_fmt_mc(p['liquidity'])} | æŒæœ‰äºº: {p['holders']:,}")
        try:
            chg = float(p['price_change_1h']) if p['price_change_1h'] else 0
        except (ValueError, TypeError):
            chg = 0
        chg_str = f"+{chg:.1f}%" if chg > 0 else f"{chg:.1f}%"
        plines.append(f"å¹´é¾„: {p['age_hours']}h | 1h: {chg_str} | æ¥æº: {p['source']}")
        sec = sec_tags_fn(p)
        if sec:
            plines.append(f"ğŸ”’ {' | '.join(sec)}")
        if show_kw and p.get('ai_keywords'):
            plines.append(f"å…³é”®è¯: {', '.join(p['ai_keywords'])}")
        if p.get('website'):
            plines.append(f"ğŸŒ {p['website']}")
        if p.get('twitter'):
            plines.append(f"ğŸ¦ @{p['twitter']}")
        if p.get('telegram'):
            plines.append(f"ğŸ’¬ {p['telegram']}")
        plines.append(f"ğŸ”— gmgn.ai/base/token/{p['address']}")
        if warns:
            for w in warns:
                plines.append(f"âš ï¸ {w}")
        return "\n".join(plines)

    if new_projects:
        lines.append("")
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append(f"ğŸ†• æ–°é¡¹ç›® ({len(new_projects)})")
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append("")
        for i, p in enumerate(new_projects, 1):
            lines.append(_fmt_project(i, p, show_kw=p.get('is_ai_mining', False)))
            lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    if ai_projects:
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append(f"ğŸ¤– AI æŒ–çŸ¿é¡¹ç›® ({len(ai_projects)})")
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append("")
        for i, p in enumerate(ai_projects, 1):
            lines.append(_fmt_project(i, p, show_kw=True))
            lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    if normal:
        lines.append("")
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append(f"ğŸ“Š å…¶ä»–é¡¹ç›® ({len(normal)})")
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append("")
        for i, p in enumerate(normal, 1):
            lines.append(_fmt_project(i, p))
            lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    if fake_mc:
        lines.append("")
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append(f"âš ï¸ ç–‘ä¼¼å‡å¸‚å€¼ ({len(fake_mc)})")
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append("")
        for i, p in enumerate(fake_mc, 1):
            lines.append(_fmt_project(i, p, show_kw=p.get('is_ai_mining', False)))
            lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    with open(out_path, 'w') as f:
        f.write("\n".join(lines))

    # æŒ‰åˆ†ç±»æ‹†åˆ†ä¸ºå¤šä¸ªæ–‡ä»¶ï¼ˆTelegram 4096å­—ç¬¦é™åˆ¶ï¼‰
    parts = []
    # Part 0: å¤´éƒ¨ + æ–°é¡¹ç›®
    p0 = []
    p0.append(f"ğŸ“‹ 48å°æ—¶å›æµ‹ç»“æœï¼ˆ{start.strftime('%m-%d')} ~ {now.strftime('%m-%d')}ï¼‰")
    p0.append(f"")
    p0.append(summary)
    if new_projects:
        p0.append("")
        p0.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        p0.append(f"ğŸ†• æ–°é¡¹ç›® ({len(new_projects)})")
        p0.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        p0.append("")
        for i, p in enumerate(new_projects, 1):
            p0.append(_fmt_project(i, p, show_kw=p.get('is_ai_mining', False)))
            p0.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    parts.append("\n".join(p0))

    # Part 1: AIæŒ–çŸ¿
    if ai_projects:
        p1 = []
        p1.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        p1.append(f"ğŸ¤– AI æŒ–çŸ¿é¡¹ç›® ({len(ai_projects)})")
        p1.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        p1.append("")
        for i, p in enumerate(ai_projects, 1):
            p1.append(_fmt_project(i, p, show_kw=True))
            p1.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        parts.append("\n".join(p1))

    # Part 2: å…¶ä»–é¡¹ç›®
    if normal:
        p2 = []
        p2.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        p2.append(f"ğŸ“Š å…¶ä»–é¡¹ç›® ({len(normal)})")
        p2.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        p2.append("")
        for i, p in enumerate(normal, 1):
            p2.append(_fmt_project(i, p))
            p2.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        parts.append("\n".join(p2))

    # Part 3: ç–‘ä¼¼å‡å¸‚å€¼
    if fake_mc:
        p3 = []
        p3.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        p3.append(f"âš ï¸ ç–‘ä¼¼å‡å¸‚å€¼ ({len(fake_mc)})")
        p3.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        p3.append("")
        for i, p in enumerate(fake_mc, 1):
            p3.append(_fmt_project(i, p, show_kw=p.get('is_ai_mining', False)))
            p3.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        parts.append("\n".join(p3))

    # å¦‚æœæŸä¸ª part è¶…è¿‡3800å­—ç¬¦ï¼Œå†æ‹†
    final_parts = []
    for part in parts:
        if len(part) <= 3800:
            final_parts.append(part)
        else:
            # æŒ‰åˆ†éš”çº¿æ‹†
            chunks = part.split("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            buf = ""
            for chunk in chunks:
                test = buf + chunk + "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
                if len(test) > 3800 and buf:
                    final_parts.append(buf.rstrip())
                    buf = chunk + "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
                else:
                    buf = test
            if buf.strip():
                final_parts.append(buf.rstrip())

    # å†™åˆ†æ®µæ–‡ä»¶
    for idx, part in enumerate(final_parts):
        with open(f"/tmp/backtest_report_p{idx+1}.txt", 'w') as f:
            f.write(part)
    # å†™æ€»æ•°æ–‡ä»¶
    with open("/tmp/backtest_report_parts.txt", 'w') as f:
        f.write(str(len(final_parts)))


if __name__ == '__main__':
    main()
