#!/usr/bin/env python3
"""
GMGN Base Chain Monitor - å¤šæ•°æ®æºé¡¹ç›®ç›‘æ§
æ•°æ®æºï¼š
  1. GMGN rank API (graduated) - ä¸»æ‰«æ
  2. GMGN new_pairs API - è¡¥å……æ‰«æ
  3. DexScreener search API - ç¬¬ä¸‰æ•°æ®æºï¼Œè¦†ç›– GMGN æ¼æ‰çš„é¡¹ç›®
æ¯10åˆ†é’Ÿæ‰«æï¼Œç­›é€‰æœ‰ä»·å€¼çš„é¡¹ç›®å¹¶é€šçŸ¥ç”¨æˆ·ã€‚
é‡ç‚¹æ ‡æ³¨ AI æŒ–çŸ¿ç±»é¡¹ç›®ã€‚
"""

import json
import time
import re
import requests
import os
import sys
import subprocess
from datetime import datetime
from collections import Counter, defaultdict

# === é…ç½® ===
CHAIN = "base"
SCAN_INTERVAL = 600  # 10åˆ†é’Ÿ
STATE_FILE = "/tmp/gmgn_monitor_state.json"
NOTIFY_FILE = "/tmp/gmgn_notify.json"
ALERT_FILE = "/tmp/gmgn_alert.json"
FAV_FILE = "/tmp/gmgn_favorites.json"
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ARCHIVE_DIR = os.path.join(os.path.dirname(SCRIPT_DIR), "archive")
ARCHIVE_DB_FILE = os.path.join(ARCHIVE_DIR, "archive_db.json")
INDEX_FILE = os.path.join(ARCHIVE_DIR, "INDEX.md")
REPORT_FILE = os.path.join(ARCHIVE_DIR, "REPORT_48H.md")
GMGN_TOKEN_URL = "https://gmgn.ai/base/token/"

# AI æŒ–çŸ¿å…³é”®è¯
AI_MINING_KEYWORDS = [
    "mine", "miner", "mining", "bot", "agent", "ai",
    "earn", "farm", "stake", "proof", "compute",
    "gpu", "hash", "reward", "epoch", "node",
    "botcoin", "agentcoin", "aibot", "automine"
]

# DexScreener æœç´¢å…³é”®è¯ï¼ˆç”¨äºå‘ç° GMGN æ¼æ‰çš„é¡¹ç›®ï¼‰
DEXSCREENER_KEYWORDS = [
    "botcoin", "mining", "miner", "ai agent", "bot coin",
    "agent coin", "compute", "gpu", "hash", "proof",
    "node", "earn", "farm", "stake", "reward",
]

GMGN_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Referer": "https://gmgn.ai/?chain=base",
    "Accept": "application/json",
    "Origin": "https://gmgn.ai"
}

DEXSCREENER_HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Accept": "application/json",
}

# è´¨é‡è¿‡æ»¤é—¨æ§›
MIN_LIQUIDITY = 5000       # æœ€ä½æµåŠ¨æ€§ $5k
MIN_HOLDERS = 20           # æœ€ä½æŒæœ‰äººæ•°
MAX_AGE_HOURS = 72         # æœ€å¤§é¡¹ç›®å¹´é¾„

# æ’é™¤çš„ä¸»æµå¸/ç¨³å®šå¸ï¼ˆä¸éœ€è¦ç›‘æ§ï¼‰
EXCLUDED_SYMBOLS = {
    "cbbtc", "weth", "usdc", "usdt", "dai", "wbtc", "eth",
    "usdbc", "aero", "degen", "brett", "toshi",
}


def log(msg):
    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f'[{ts}] {msg}', flush=True)


def load_state():
    try:
        with open(STATE_FILE, 'r') as f:
            return json.load(f)
    except Exception:
        return {'notified_tokens': {}, 'last_scan': 0}


def save_state(state):
    now = int(time.time())
    # æ¸…ç†72å°æ—¶å‰çš„è®°å½•
    expired_addrs = {
        k for k, v in state['notified_tokens'].items()
        if now - v >= 72 * 3600
    }
    state['notified_tokens'] = {
        k: v for k, v in state['notified_tokens'].items()
        if k not in expired_addrs
    }
    # åŒæ­¥æ¸…ç† notified_full
    if 'notified_full' in state:
        for addr in expired_addrs:
            state['notified_full'].pop(addr, None)
    # åŸå­å†™å…¥ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶å† renameï¼Œé˜²æ­¢è¿›ç¨‹è¢«killå¯¼è‡´æŸå
    tmp_file = STATE_FILE + '.tmp'
    with open(tmp_file, 'w') as f:
        json.dump(state, f)
    os.rename(tmp_file, STATE_FILE)



def is_ai_mining(text_parts):
    """æ£€æµ‹æ˜¯å¦ä¸º AI æŒ–çŸ¿ç±»é¡¹ç›®ï¼Œtext_parts æ˜¯å¾…æ£€æµ‹çš„å­—ç¬¦ä¸²åˆ—è¡¨
    å¯¹ symbolï¼ˆç¬¬ä¸€ä¸ªå…ƒç´ ï¼‰ä½¿ç”¨å…¨è¯åŒ¹é…ï¼Œå¯¹ website/twitter ä½¿ç”¨å…¨è¯åŒ¹é…ï¼ˆå«URLåˆ†éš”ç¬¦ï¼‰"""
    if not text_parts:
        return False, []
    symbol = (text_parts[0] or '').lower()
    rest = ' '.join(t.lower() for t in text_parts[1:] if t)
    # å°† URL åˆ†éš”ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä½¿å…¨è¯åŒ¹é…èƒ½è¯†åˆ« URL è·¯å¾„ä¸­çš„è¯
    rest = re.sub(r'[/\-_\.:]', ' ', rest)
    matches = []
    for kw in AI_MINING_KEYWORDS:
        # symbol ç”¨å…¨è¯åŒ¹é…
        if re.search(r'\b' + re.escape(kw) + r'\b', symbol) or kw == symbol:
            matches.append(kw)
        # website/twitter ç”¨å…¨è¯åŒ¹é…ï¼ˆURLå·²åˆ†è¯ï¼‰
        elif rest and re.search(r'\b' + re.escape(kw) + r'\b', rest):
            matches.append(kw)
    return len(matches) > 0, matches


# ============================================================
# æ•°æ®æº 1: GMGN rank API (graduated)
# ============================================================
def fetch_gmgn_graduated(limit=100):
    """è·å– GMGN å·²å¼€ç›˜(graduated)é¡¹ç›®"""
    url = f"https://gmgn.ai/defi/quotation/v1/rank/{CHAIN}/swaps/1h"
    params = {
        "limit": limit,
        "orderby": "open_timestamp",
        "direction": "desc",
        "tag": "graduated"
    }
    try:
        resp = requests.get(url, params=params, headers=GMGN_HEADERS, timeout=15)
        data = resp.json()
        if data.get('code') == 0:
            tokens = data['data']['rank']
            log(f"[GMGN-rank] è·å– {len(tokens)} ä¸ª graduated é¡¹ç›®")
            return tokens
        log(f"[GMGN-rank] API error: {data.get('msg')}")
    except Exception as e:
        log(f"[GMGN-rank] Fetch error: {e}")
    return []


def parse_gmgn_rank_token(t):
    """å°† GMGN rank token è½¬ä¸ºç»Ÿä¸€æ ¼å¼"""
    now = int(time.time())
    age_hours = (now - (t.get('open_timestamp') or 0)) / 3600
    return {
        'address': (t.get('address') or '').lower(),
        'symbol': t.get('symbol', '?'),
        'price': t.get('price', 0),
        'market_cap': float(t.get('market_cap') or 0),
        'liquidity': float(t.get('liquidity') or 0),
        'volume_1h': float(t.get('volume') or 0),
        'swaps': int(t.get('swaps') or 0),
        'buys': int(t.get('buys') or 0),
        'sells': int(t.get('sells') or 0),
        'holders': int(t.get('holder_count') or 0),
        'price_change_1h': t.get('price_change_percent1h', 0),
        'age_hours': round(age_hours, 1),
        'open_timestamp': t.get('open_timestamp', 0),
        'twitter': t.get('twitter_username') or '',
        'website': t.get('website') or '',
        'telegram': t.get('telegram') or '',
        'is_honeypot': t.get('is_honeypot', 0),
        'buy_tax': t.get('buy_tax', '0'),
        'sell_tax': t.get('sell_tax', '0'),
        'renounced': t.get('renounced', 0),
        'smart_buy_24h': t.get('smart_buy_24h', 0),
        'smart_sell_24h': t.get('smart_sell_24h', 0),
        'source': 'gmgn_rank',
    }


# ============================================================
# æ•°æ®æº 2: GMGN new_pairs API
# ============================================================
def fetch_gmgn_new_pairs(limit=100):
    """è·å– GMGN æ–°äº¤æ˜“å¯¹ï¼Œè¡¥å…… rank æ¼æ‰çš„é¡¹ç›®"""
    url = f"https://gmgn.ai/defi/quotation/v1/pairs/{CHAIN}/new_pairs"
    params = {
        "limit": limit,
        "orderby": "open_timestamp",
        "direction": "desc",
    }
    try:
        resp = requests.get(url, params=params, headers=GMGN_HEADERS, timeout=15)
        data = resp.json()
        if data.get('code') == 0:
            pairs = data['data'].get('pairs', [])
            log(f"[GMGN-pairs] è·å– {len(pairs)} ä¸ªæ–°äº¤æ˜“å¯¹")
            return pairs
        log(f"[GMGN-pairs] API error: {data.get('msg')}")
    except Exception as e:
        log(f"[GMGN-pairs] Fetch error: {e}")
    return []


def parse_gmgn_pair(p):
    """å°† GMGN new_pair è½¬ä¸ºç»Ÿä¸€æ ¼å¼"""
    bti = p.get('base_token_info', {})
    now = int(time.time())
    open_ts = p.get('open_timestamp') or 0
    age_hours = (now - open_ts) / 3600 if open_ts else 0

    social = bti.get('social_links', {}) or {}
    return {
        'address': (bti.get('address') or '').lower(),
        'symbol': bti.get('symbol', '?'),
        'price': bti.get('price', 0),
        'market_cap': float(bti.get('market_cap') or 0),
        'liquidity': float(bti.get('liquidity') or 0),
        'volume_1h': float(bti.get('volume') or 0),
        'swaps': int(bti.get('swaps') or 0),
        'buys': int(bti.get('buys') or 0),
        'sells': int(bti.get('sells') or 0),
        'holders': int(bti.get('holder_count') or 0),
        'price_change_1h': bti.get('price_change_percent1h', 0),
        'age_hours': round(age_hours, 1),
        'open_timestamp': open_ts,
        'twitter': social.get('twitter_username') or '',
        'website': social.get('website') or '',
        'telegram': social.get('telegram') or '',
        'is_honeypot': bti.get('is_honeypot', 0),
        'buy_tax': bti.get('buy_tax', '0'),
        'sell_tax': bti.get('sell_tax', '0'),
        'renounced': bti.get('renounced', 0),
        'smart_buy_24h': 0,
        'smart_sell_24h': 0,
        'source': 'gmgn_pairs',
    }


# ============================================================
# æ•°æ®æº 3: DexScreener search API
# ============================================================
def fetch_dexscreener():
    """ç”¨å…³é”®è¯æœç´¢ DexScreenerï¼Œå‘ç° GMGN æ¼æ‰çš„ Base é“¾é¡¹ç›®"""
    all_tokens = {}
    for kw in DEXSCREENER_KEYWORDS:
        try:
            resp = requests.get(
                f'https://api.dexscreener.com/latest/dex/search?q={kw}',
                headers=DEXSCREENER_HEADERS, timeout=15
            )
            if resp.status_code == 429:
                log(f"[DexScreener] é™æµï¼Œæš‚åœ30s")
                time.sleep(30)
                continue
            if resp.status_code != 200:
                continue
            d = resp.json()
            pairs = [p for p in d.get('pairs', []) if p.get('chainId') == 'base']
            for p in pairs:
                addr = (p.get('baseToken', {}).get('address') or '').lower()
                if addr and addr not in all_tokens:
                    all_tokens[addr] = p
            time.sleep(0.5)  # é¿å…é™æµï¼ˆ15ä¸ªå…³é”®è¯ï¼Œæ€»è®¡~7.5sï¼‰
        except Exception as e:
            log(f"[DexScreener] search '{kw}' error: {e}")
    log(f"[DexScreener] å…³é”®è¯æœç´¢è·å– {len(all_tokens)} ä¸ª Base é“¾é¡¹ç›®")
    return list(all_tokens.values())


def parse_dexscreener_pair(p):
    """å°† DexScreener pair è½¬ä¸ºç»Ÿä¸€æ ¼å¼"""
    bt = p.get('baseToken', {})
    now_ms = time.time() * 1000
    created = p.get('pairCreatedAt') or 0
    age_hours = (now_ms - created) / 3600000 if created else 0

    txns_1h = p.get('txns', {}).get('h1', {})
    info = p.get('info', {})
    websites = info.get('websites', [])
    socials = info.get('socials', [])

    twitter = ''
    website = ''
    telegram = ''
    for s in socials:
        if s.get('type') == 'twitter':
            url = s.get('url', '')
            twitter = url.split('/')[-1] if '/' in url else url
        elif s.get('type') == 'telegram':
            telegram = s.get('url', '')
    if websites:
        website = websites[0].get('url', '')

    return {
        'address': (bt.get('address') or '').lower(),
        'symbol': bt.get('symbol', '?'),
        'price': float(p.get('priceUsd') or 0),
        'market_cap': float(p.get('marketCap') or 0),
        'liquidity': float((p.get('liquidity') or {}).get('usd') or 0),
        'volume_1h': float((p.get('volume') or {}).get('h1') or 0),
        'swaps': int(txns_1h.get('buys', 0)) + int(txns_1h.get('sells', 0)),
        'buys': int(txns_1h.get('buys', 0)),
        'sells': int(txns_1h.get('sells', 0)),
        'holders': 0,  # DexScreener ä¸æä¾› holder æ•°æ®
        'price_change_1h': float((p.get('priceChange') or {}).get('h1') or 0),
        'age_hours': round(age_hours, 1),
        'open_timestamp': int(created / 1000) if created else 0,
        'twitter': twitter,
        'website': website,
        'telegram': telegram,
        'is_honeypot': 0,
        'buy_tax': '0',
        'sell_tax': '0',
        'renounced': 0,
        'smart_buy_24h': 0,
        'smart_sell_24h': 0,
        'source': 'dexscreener',
    }


# ============================================================
# åˆå¹¶ã€è¿‡æ»¤ã€é€šçŸ¥
# ============================================================
def merge_tokens(sources):
    """åˆå¹¶å¤šä¸ªæ•°æ®æºï¼Œå»é‡ï¼ˆåŒåœ°å€ä¿ç•™æ•°æ®æ›´ä¸°å¯Œçš„æºï¼‰"""
    merged = {}
    # ä¼˜å…ˆçº§ï¼šgmgn_rank > gmgn_pairs > dexscreener
    priority = {'gmgn_rank': 3, 'gmgn_pairs': 2, 'dexscreener': 1}
    for token in sources:
        addr = token.get('address', '')
        if not addr:
            continue
        existing = merged.get(addr)
        if not existing:
            merged[addr] = token
        else:
            # ä¿ç•™ä¼˜å…ˆçº§æ›´é«˜çš„æº
            if priority.get(token['source'], 0) > priority.get(existing['source'], 0):
                merged[addr] = token
            # å¦‚æœ DexScreener æœ‰ holder æ•°æ®è¡¥å……
            elif existing.get('holders', 0) == 0 and token.get('holders', 0) > 0:
                existing['holders'] = token['holders']
    return list(merged.values())


def filter_quality(tokens):
    """è´¨é‡è¿‡æ»¤"""
    results = []
    for t in tokens:
        # æ’é™¤ä¸»æµå¸/ç¨³å®šå¸
        if t['symbol'].lower() in EXCLUDED_SYMBOLS:
            continue
        # å¹´é¾„è¿‡æ»¤
        if t['age_hours'] > MAX_AGE_HOURS:
            continue
        # æµåŠ¨æ€§è¿‡æ»¤
        if t['liquidity'] < MIN_LIQUIDITY:
            continue
        # æŒæœ‰äººè¿‡æ»¤ï¼ˆDexScreener æ²¡æœ‰ holder æ•°æ®ï¼Œæ”¾å®½ï¼‰
        if t['holders'] > 0 and t['holders'] < MIN_HOLDERS:
            continue
        results.append(t)
    return results


def enrich_ai_mining(tokens):
    """æ ‡è®° AI æŒ–çŸ¿é¡¹ç›®"""
    for t in tokens:
        text_parts = [t['symbol'], t['website'], t['twitter']]
        is_ai, keywords = is_ai_mining(text_parts)
        t['is_ai_mining'] = is_ai
        t['ai_keywords'] = keywords
        # å¸‚å€¼/æµåŠ¨æ€§æ¯”å€¼
        liq = t.get('liquidity', 0)
        mc = t.get('market_cap', 0)
        t['mc_liq_ratio'] = round(mc / liq, 1) if liq > 0 else 0
        # æµåŠ¨æ€§çº§åˆ«: red(<10k), yellow(10k-20k), normal(>20k)
        if liq < 10000:
            t['liq_level'] = 'red'
        elif liq < 20000:
            t['liq_level'] = 'yellow'
        else:
            t['liq_level'] = 'normal'
    return tokens


# ============================================================
# åŒåä»£å¸è¯„åˆ†ç³»ç»Ÿ
# ============================================================
def _fetch_dexscreener_creation(address):
    """ç”¨ DexScreener è·å–ä»£å¸æœ€æ—©åˆ›å»ºæ—¶é—´"""
    try:
        resp = requests.get(
            f'https://api.dexscreener.com/latest/dex/tokens/{address}',
            headers=DEXSCREENER_HEADERS, timeout=10
        )
        if resp.status_code == 200:
            pairs = resp.json().get('pairs', [])
            if pairs:
                # å–æ‰€æœ‰æ± å­ä¸­æœ€æ—©çš„åˆ›å»ºæ—¶é—´
                timestamps = [p.get('pairCreatedAt', 0) for p in pairs if p.get('pairCreatedAt', 0) > 0]
                if timestamps:
                    return min(timestamps)
    except Exception as e:
        log(f"[è¯„åˆ†] DexScreener æŸ¥è¯¢ {address[:10]}... å¤±è´¥: {e}")
    return 0


def score_single_token(t):
    """å¯¹å•ä¸ªé¡¹ç›®æ‰“åŸºç¡€åˆ†ï¼ˆæ— åŒåå¯¹æ¯”æ—¶ä½¿ç”¨ï¼‰"""
    score = 0
    # æœ‰ Twitter/Website +3
    if bool(t.get('twitter')) or bool(t.get('website')):
        score += 3
    # åˆçº¦å·²éªŒè¯(renounced) +2
    if t.get('renounced'):
        score += 2
    # Smart money ä¹°å…¥ +3
    if t.get('smart_buy_24h', 0) > 0:
        score += 3
    # æµåŠ¨æ€§æƒ©ç½šï¼ˆæŒ‰å¹´é¾„åˆ†çº§ï¼‰
    liq = t.get('liquidity', 0)
    age = t.get('age_hours', 0) or 0
    if age < 1:
        if liq < 10000: score -= 1
    elif age < 24:
        if liq < 10000: score -= 2
        elif liq < 20000: score -= 1
    elif age < 48:
        if liq < 10000: score -= 4
        elif liq < 20000: score -= 2
    else:
        if liq < 10000: score -= 6
        elif liq < 20000: score -= 3
    # ä¹°å–æ¯”å¼‚å¸¸æ£€æµ‹ï¼ˆç–‘ä¼¼è²”è²…ï¼‰
    buys = int(t.get('buys', 0) or 0)
    sells = int(t.get('sells', 0) or 0)
    if buys > 50 and sells > 0 and buys / sells >= 3:
        t['suspect_honeypot'] = True
        score -= 3
    elif buys > 50 and sells == 0:
        t['suspect_honeypot'] = True
        score -= 3
    # ç¡®è®¤èœœç½/é«˜ç¨ç‡
    if t.get('is_honeypot') == 1:
        score -= 5
    else:
        try:
            st = float(t.get('sell_tax', 0) or 0)
            if st >= 50:
                score -= 4
            elif st >= 20:
                score -= 2
        except (ValueError, TypeError):
            pass
    t['trust_score'] = score
    t['trust_rank'] = ''
    return t


def score_duplicate_tokens(duplicates):
    """
    å¯¹åŒåä»£å¸ç»„æ‰“åˆ†ã€‚
    è¯„åˆ†è¡¨:
      éƒ¨ç½²æ—¶é—´æœ€æ—©  +3
      æµåŠ¨æ€§æœ€é«˜    +2
      æŒæœ‰äººæœ€å¤š    +2
      æœ‰Twitter/Website +3
      åˆçº¦å·²éªŒè¯(renounced) +2
      Smart moneyä¹°å…¥ +3
    è¿”å›æ’åºåçš„åˆ—è¡¨ï¼ˆé«˜åˆ†åœ¨å‰ï¼‰ï¼Œæ¯ä¸ªtokené™„å¸¦ 'trust_score' å’Œ 'trust_rank'
    """
    if len(duplicates) <= 1:
        for t in duplicates:
            t['trust_score'] = 0
            t['trust_rank'] = ''
        return duplicates

    # è¡¥å…¨åˆ›å»ºæ—¶é—´ï¼šå¦‚æœ open_timestamp ä¸º 0 æˆ–ç¼ºå¤±ï¼Œç”¨ DexScreener æŸ¥ï¼ˆé™åˆ¶æœ€å¤š5æ¬¡APIè°ƒç”¨ï¼‰
    api_calls = 0
    for t in duplicates:
        if not t.get('open_timestamp') or t['open_timestamp'] < 1000000000:
            if api_calls >= 5:
                log(f"[è¯„åˆ†] åŒåç»„APIè°ƒç”¨è¾¾ä¸Šé™ï¼Œè·³è¿‡å‰©ä½™è¡¥å…¨")
                break
            ts = _fetch_dexscreener_creation(t['address'])
            if ts:
                t['open_timestamp'] = int(ts / 1000) if ts > 1e12 else int(ts)
            api_calls += 1
            time.sleep(0.5)

    # æ‰¾å„ç»´åº¦æœ€ä¼˜å€¼
    valid_ts = [t['open_timestamp'] for t in duplicates if t.get('open_timestamp', 0) > 1000000000]
    earliest_ts = min(valid_ts) if valid_ts else 0
    max_liq = max((t.get('liquidity', 0) for t in duplicates), default=0)
    max_holders = max((t.get('holders', 0) for t in duplicates), default=0)
    max_smart = max((t.get('smart_buy_24h', 0) for t in duplicates), default=0)

    for t in duplicates:
        score = 0

        # éƒ¨ç½²æ—¶é—´æœ€æ—© +3
        ts = t.get('open_timestamp', 0)
        if earliest_ts > 0 and ts > 0 and ts == earliest_ts:
            score += 3

        # æµåŠ¨æ€§æœ€é«˜ +2
        liq = t.get('liquidity', 0)
        if max_liq > 0 and liq == max_liq:
            score += 2

        # æŒæœ‰äººæœ€å¤š +2
        holders = t.get('holders', 0)
        if max_holders > 0 and holders == max_holders:
            score += 2

        # æœ‰ Twitter/Website +3
        has_social = bool(t.get('twitter')) or bool(t.get('website'))
        if has_social:
            score += 3

        # åˆçº¦å·²éªŒè¯(renounced) +2
        if t.get('renounced'):
            score += 2

        # Smart money ä¹°å…¥ +3
        smart = t.get('smart_buy_24h', 0)
        if max_smart > 0 and smart == max_smart:
            score += 3

        # æµåŠ¨æ€§è¿‡ä½æƒ©ç½šï¼ˆæŒ‰å¹´é¾„åˆ†çº§ï¼‰
        age = t.get('age_hours', 0) or 0
        if age < 1:
            if liq < 10000: score -= 1
        elif age < 24:
            if liq < 10000: score -= 2
            elif liq < 20000: score -= 1
        elif age < 48:
            if liq < 10000: score -= 4
            elif liq < 20000: score -= 2
        else:
            if liq < 10000: score -= 6
            elif liq < 20000: score -= 3

        # ä¹°å–æ¯”å¼‚å¸¸æ£€æµ‹ï¼ˆç–‘ä¼¼è²”è²…ï¼‰
        buys = int(t.get('buys', 0) or 0)
        sells = int(t.get('sells', 0) or 0)
        if buys > 50 and sells > 0 and buys / sells >= 3:
            t['suspect_honeypot'] = True
            score -= 3
        elif buys > 50 and sells == 0:
            t['suspect_honeypot'] = True
            score -= 3
        # ç¡®è®¤èœœç½/é«˜ç¨ç‡
        if t.get('is_honeypot') == 1:
            score -= 5
        else:
            try:
                st = float(t.get('sell_tax', 0) or 0)
                if st >= 50:
                    score -= 4
                elif st >= 20:
                    score -= 2
            except (ValueError, TypeError):
                pass

        t['trust_score'] = score

    # æ’åºï¼šé«˜åˆ†åœ¨å‰
    duplicates.sort(key=lambda x: -x['trust_score'])

    # æ ‡æ³¨ rank
    top_score = duplicates[0]['trust_score']
    for i, t in enumerate(duplicates):
        if i == 0 and t['trust_score'] > duplicates[-1]['trust_score']:
            if t['trust_score'] >= 7:
                t['trust_rank'] = 'âœ…å¯èƒ½çœŸå“'
            else:
                t['trust_rank'] = 'âš ï¸å¾…éªŒè¯'
        elif t['trust_score'] == top_score:
            t['trust_rank'] = 'âš ï¸å¾…éªŒè¯'
        else:
            t['trust_rank'] = 'âŒå¯èƒ½ä»¿ç›˜'

    return duplicates


def detect_and_score_duplicates(new_projects, state):
    """
    æ£€æµ‹æ–°é¡¹ç›®ä¸­æ˜¯å¦æœ‰åŒåä»£å¸ï¼ˆä¸æœ¬è½®å…¶ä»–æ–°é¡¹ç›® + å†å²å·²é€šçŸ¥é¡¹ç›®å¯¹æ¯”ï¼‰ã€‚
    å¯¹åŒåç»„è¿›è¡Œè¯„åˆ†ï¼Œç»™æ¯ä¸ªé¡¹ç›®é™„åŠ  trust_score å’Œ trust_rankã€‚
    """
    # æ„å»º symbol -> [tokens] æ˜ å°„ï¼ˆæ–°é¡¹ç›® + å†å²ï¼ŒæŒ‰åœ°å€å»é‡ï¼‰
    symbol_groups = defaultdict(dict)  # symbol -> {addr: token}

    # å†å²å·²é€šçŸ¥é¡¹ç›®
    for addr, full in state.get('notified_full', {}).items():
        if full:
            sym = full.get('symbol', '').upper()
            if sym:
                symbol_groups[sym][addr] = full

    # æœ¬è½®æ–°é¡¹ç›®ï¼ˆè¦†ç›–å†å²ä¸­çš„åŒåœ°å€æ•°æ®ï¼‰
    for t in new_projects:
        sym = t.get('symbol', '').upper()
        if sym:
            symbol_groups[sym][t['address']] = t

    # æ‰¾å‡ºæœ‰åŒåçš„ symbol
    dup_symbols = {sym for sym, tokens in symbol_groups.items() if len(tokens) > 1}

    if not dup_symbols:
        # æ— åŒåï¼Œæ‰€æœ‰æ–°é¡¹ç›®æ‰“åŸºç¡€åˆ†
        for t in new_projects:
            score_single_token(t)
        return new_projects

    log(f"[è¯„åˆ†] æ£€æµ‹åˆ° {len(dup_symbols)} ä¸ªåŒå symbol: {', '.join(sorted(dup_symbols))}")

    # å¯¹æ¯ä¸ªåŒåç»„è¯„åˆ†
    scored_addrs = {}
    for sym in dup_symbols:
        group = list(symbol_groups[sym].values())
        scored = score_duplicate_tokens(group)
        for t in scored:
            scored_addrs[t['address']] = t
        scores_str = ', '.join(f"{t['address'][:8]}={t['trust_score']}({t['trust_rank']})" for t in scored)
        log(f"[è¯„åˆ†] {sym}: {scores_str}")

    # æ›´æ–°æ–°é¡¹ç›®çš„è¯„åˆ†
    for t in new_projects:
        if t['address'] in scored_addrs:
            s = scored_addrs[t['address']]
            t['trust_score'] = s['trust_score']
            t['trust_rank'] = s['trust_rank']
        else:
            # ä¸åœ¨åŒåç»„é‡Œï¼Œæ‰“åŸºç¡€åˆ†
            score_single_token(t)

    # åŒæ—¶æ›´æ–° state ä¸­å†å²é¡¹ç›®çš„è¯„åˆ†
    for addr, s in scored_addrs.items():
        if addr in state.get('notified_full', {}):
            state['notified_full'][addr]['trust_score'] = s['trust_score']
            state['notified_full'][addr]['trust_rank'] = s['trust_rank']

    return new_projects


def process_all(notified_set, state=None):
    """ä»ä¸‰ä¸ªæ•°æ®æºè·å–ã€åˆå¹¶ã€è¿‡æ»¤é¡¹ç›®"""
    all_parsed = []

    # æ•°æ®æº 1: GMGN graduated
    for t in fetch_gmgn_graduated():
        all_parsed.append(parse_gmgn_rank_token(t))

    # æ•°æ®æº 2: GMGN new_pairs
    for p in fetch_gmgn_new_pairs():
        all_parsed.append(parse_gmgn_pair(p))

    # æ•°æ®æº 3: DexScreener
    for p in fetch_dexscreener():
        all_parsed.append(parse_dexscreener_pair(p))

    log(f"[åˆå¹¶] ä¸‰ä¸ªæºå…± {len(all_parsed)} æ¡åŸå§‹æ•°æ®")

    # åˆå¹¶å»é‡
    merged = merge_tokens(all_parsed)
    log(f"[åˆå¹¶] å»é‡å {len(merged)} ä¸ªå”¯ä¸€é¡¹ç›®")

    # äº¤å‰è¡¥å…¨ notified_full ä¸­ open_timestamp=0 çš„é¡¹ç›®
    if state:
        merged_by_addr = {t['address']: t for t in merged}
        patched = 0
        for addr, full in state.get('notified_full', {}).items():
            if not full.get('open_timestamp') and addr in merged_by_addr:
                new_ots = merged_by_addr[addr].get('open_timestamp', 0)
                if new_ots and new_ots > 1000000000:
                    full['open_timestamp'] = new_ots
                    full['age_hours'] = round((time.time() - new_ots) / 3600, 1)
                    patched += 1
        if patched:
            log(f"[è¡¥å…¨] äº¤å‰éªŒè¯ä¿®å¤äº† {patched} ä¸ªé¡¹ç›®çš„ open_timestamp")
            save_state(state)

    # è¿‡æ»¤å·²é€šçŸ¥çš„
    new_tokens = [t for t in merged if t['address'] not in notified_set]
    log(f"[è¿‡æ»¤] æ’é™¤å·²é€šçŸ¥å {len(new_tokens)} ä¸ª")

    # æ ¡éªŒæ–°é¡¹ç›®çš„ open_timestampï¼ˆå–æœ€æ—©æ± å­åˆ›å»ºæ—¶é—´ï¼Œé™åˆ¶æœ€å¤š20æ¬¡APIè°ƒç”¨ï¼‰
    api_calls = 0
    for t in new_tokens:
        if api_calls >= 20:
            log(f"[æ ¡éªŒ] æ–°é¡¹ç›®APIè°ƒç”¨è¾¾ä¸Šé™({api_calls})ï¼Œè·³è¿‡å‰©ä½™")
            break
        dex_ts = _fetch_dexscreener_creation(t['address'])
        if dex_ts:
            dex_ts_sec = int(dex_ts / 1000) if dex_ts > 1e12 else int(dex_ts)
            cur_ts = t.get('open_timestamp', 0)
            if not cur_ts or cur_ts > dex_ts_sec:
                t['open_timestamp'] = dex_ts_sec
                t['age_hours'] = round((time.time() - dex_ts_sec) / 3600, 1)
        api_calls += 1
        time.sleep(0.3)

    # è´¨é‡è¿‡æ»¤
    quality = filter_quality(new_tokens)
    log(f"[è¿‡æ»¤] è´¨é‡è¿‡æ»¤å {len(quality)} ä¸ª")

    # æ ‡è®° AI æŒ–çŸ¿
    enriched = enrich_ai_mining(quality)

    # æ’åºï¼šAI æŒ–çŸ¿ä¼˜å…ˆï¼Œç„¶åæŒ‰å¼€ç›˜æ—¶é—´å€’åº
    enriched.sort(key=lambda x: (not x['is_ai_mining'], -x['open_timestamp']))

    return enriched, merged


def notify(projects):
    """å†™å…¥é€šçŸ¥æ–‡ä»¶å¹¶å”¤é†’ AI agentï¼Œé€šçŸ¥æ ¼å¼å¸¦ GMGN é“¾æ¥å’Œè¯„åˆ†"""
    # ç»™æ¯ä¸ªé¡¹ç›®åŠ ä¸Š gmgn é“¾æ¥
    for p in projects:
        p['gmgn_url'] = f"{GMGN_TOKEN_URL}{p['address']}"

    # ç»Ÿè®¡æœ‰åŒåè¯„åˆ†çš„é¡¹ç›®
    dup_count = sum(1 for p in projects if p.get('trust_score', 0) > 0 or p.get('trust_rank'))

    notification = {
        'time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'count': len(projects),
        'ai_mining_count': sum(1 for p in projects if p['is_ai_mining']),
        'duplicate_scored_count': dup_count,
        'projects': projects
    }
    with open(NOTIFY_FILE, 'w') as f:
        json.dump(notification, f, ensure_ascii=False)

    try:
        ai_count = notification['ai_mining_count']
        text = f"é“¾ä¸Šç›‘æ§: {len(projects)} ä¸ªæ–°é¡¹ç›®"
        if ai_count > 0:
            text += f"ï¼Œå…¶ä¸­ {ai_count} ä¸ªAIæŒ–çŸ¿é¡¹ç›®ï¼"
        subprocess.Popen([
            'openclaw', 'system', 'event', '--text', text, '--mode', 'now'
        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except Exception:
        pass


# ============================================================
# å½’æ¡£ç³»ç»Ÿ
# ============================================================
def _fmt_mc(val):
    if val >= 1_000_000_000:
        return f"${val/1_000_000_000:.2f}B"
    elif val >= 1_000_000:
        return f"${val/1_000_000:.2f}M"
    elif val >= 1_000:
        return f"${val/1_000:.0f}K"
    return f"${val:.0f}"


def _fmt_liq(val):
    """æµåŠ¨æ€§æ ¼å¼åŒ–ï¼Œå¸¦é¢œè‰²æ ‡è®°"""
    formatted = _fmt_mc(val)
    if val < 10000:
        return f"ğŸ”´{formatted}"
    elif val < 20000:
        return f"ğŸŸ¡{formatted}"
    return formatted


def _fmt_project_md(p, idx, symbol_counts=None):
    ai_tag = "ğŸ¤–" if p.get('is_ai_mining') else "ğŸ“Š"
    sym = p['symbol']
    cnt = symbol_counts.get(sym, 1) if symbol_counts else 1
    if cnt > 1:
        sym = f"{sym} ({p['address'][:6]}) [åŒåÃ—{cnt}]"
    mc = p.get('market_cap', 0)
    liq = p.get('liquidity', 0)
    mc_liq_ratio = round(mc / liq, 1) if liq > 0 else 0
    lines = [
        f"### {ai_tag} #{idx} {sym}",
        f"",
        f"- åˆçº¦: `{p['address']}`",
        f"- MC: {_fmt_mc(mc)} | æµåŠ¨æ€§: {_fmt_liq(liq)} | MC/Liq: {mc_liq_ratio}x",
    ]
    if p.get('holders'):
        lines.append(f"- æŒæœ‰äºº: {p['holders']:,}")
    lines.append(f"- å¹´é¾„: {p.get('age_hours', 0)}h | æ¥æº: {p.get('source', '?')}")
    if p.get('website'):
        lines.append(f"- ğŸŒ {p['website']}")
    if p.get('twitter'):
        lines.append(f"- ğŸ¦ @{p['twitter']}")
    if p.get('telegram'):
        lines.append(f"- ğŸ’¬ {p['telegram']}")
    lines.append(f"- ğŸ”— [GMGN]({GMGN_TOKEN_URL}{p['address']})")
    if p.get('ai_keywords'):
        lines.append(f"- å…³é”®è¯: {', '.join(p['ai_keywords'])}")
    if p.get('trust_rank'):
        lines.append(f"- å¯ä¿¡åº¦: {p['trust_rank']} (è¯„åˆ†: {p.get('trust_score', 0)}/15)")
    lines.append("")
    return "\n".join(lines)


def _load_archive_db():
    if os.path.exists(ARCHIVE_DB_FILE):
        with open(ARCHIVE_DB_FILE) as f:
            return json.load(f)
    return {}


def _save_archive_db(db):
    with open(ARCHIVE_DB_FILE, 'w') as f:
        json.dump(db, f, ensure_ascii=False, indent=2)


def _update_index(db):
    # ç»Ÿè®¡å…¨å±€åŒå
    _all_symbols = Counter()
    for projects in db.values():
        for p in projects:
            _all_symbols[p['symbol']] += 1

    lines = [
        "# é“¾ä¸Šé¡¹ç›®å½’æ¡£ç´¢å¼•", "",
        f"æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}", "",
        "| æ—¥æœŸ | é¡¹ç›®æ•° | AIæŒ–çŸ¿ | é¡¹ç›®åˆ—è¡¨ |",
        "|------|--------|---------|----------|",
    ]
    total = total_ai = 0
    for date_str in sorted(db.keys(), reverse=True):
        projects = db[date_str]
        ai_count = sum(1 for p in projects if p.get('is_ai_mining'))
        total += len(projects)
        total_ai += ai_count
        def _idx_name(p):
            tag = "ğŸ¤–" if p.get('is_ai_mining') else ""
            sym = p['symbol']
            if _all_symbols.get(sym, 1) > 1:
                sym = f"{sym}({p['address'][:6]})"
            return tag + sym
        names = [_idx_name(p) for p in projects]
        names_str = ", ".join(names[:8])
        if len(names) > 8:
            names_str += f" +{len(names)-8}"
        lines.append(f"| [{date_str}]({date_str}.md) | {len(projects)} | {ai_count} | {names_str} |")
    lines += ["", f"**æ€»è®¡: {total} ä¸ªé¡¹ç›® | AIæŒ–çŸ¿: {total_ai}**", ""]

    # åˆçº¦åœ°å€ç´¢å¼•
    lines += ["## åˆçº¦åœ°å€ç´¢å¼•", "",
              "| æ—¥æœŸ | é¡¹ç›® | åˆçº¦åœ°å€ | AI |",
              "|------|------|----------|-----|"]
    for date_str in sorted(db.keys(), reverse=True):
        for p in db[date_str]:
            ai = "ğŸ¤–" if p.get('is_ai_mining') else ""
            sym = p['symbol']
            if _all_symbols.get(sym, 1) > 1:
                sym = f"{sym} ({p['address'][:6]})"
            lines.append(f"| {date_str} | {sym} | `{p['address']}` | {ai} |")
    lines.append("")
    with open(INDEX_FILE, 'w') as f:
        f.write("\n".join(lines))


def archive_and_report(state):
    """å½’æ¡£è¿‡æœŸé¡¹ç›® + ç”Ÿæˆ48hæŠ¥å‘Šã€‚ä» state ä¸­è·å–æ‰€æœ‰å·²çŸ¥é¡¹ç›®ã€‚"""
    os.makedirs(ARCHIVE_DIR, exist_ok=True)
    now = int(time.time())
    cutoff = now - 48 * 3600

    # æ”¶é›† state ä¸­æ‰€æœ‰å·²é€šçŸ¥é¡¹ç›®çš„å®Œæ•´æ•°æ®
    all_projects = []
    for addr, ts in state.get('notified_tokens', {}).items():
        # ä» notified_full è·å–å®Œæ•´æ•°æ®
        full = state.get('notified_full', {}).get(addr)
        if full:
            # é‡æ–°è®¡ç®— age
            open_ts = full.get('open_timestamp', 0)
            if open_ts:
                full['age_hours'] = round((now - open_ts) / 3600, 1)
            all_projects.append(full)

    if not all_projects:
        return

    # åˆ†ä¸ºæ´»è·ƒå’Œè¿‡æœŸ
    active = [p for p in all_projects if p.get('open_timestamp', 0) >= cutoff]
    expired = [p for p in all_projects if p.get('open_timestamp', 0) < cutoff]

    # ç”Ÿæˆ48hæŠ¥å‘Š
    active.sort(key=lambda x: (not x.get('is_ai_mining', False), -x.get('open_timestamp', 0)))
    ai_count = sum(1 for p in active if p.get('is_ai_mining'))
    # åŒåæ£€æµ‹ï¼šå½“æ‰¹ + å†å² notified_full åˆå¹¶
    _sc = Counter(p['symbol'] for p in active)
    active_addrs = {p['address'] for p in active}
    for _addr, _hp in state.get('notified_full', {}).items():
        if _addr not in active_addrs:
            _sym = _hp.get('symbol', '')
            if _sym:
                _sc[_sym] += 1

    # ç–‘ä¼¼å‡å¸‚å€¼åˆ¤æ–­
    def _is_fake_mc(p):
        liq = p.get('liquidity', 0)
        mc = p.get('market_cap', 0)
        return liq > 0 and mc / liq > 1000

    ai_list = [p for p in active if p.get('is_ai_mining') and not _is_fake_mc(p)]
    normal_list = [p for p in active if not p.get('is_ai_mining') and not _is_fake_mc(p)]
    fake_mc_list = [p for p in active if _is_fake_mc(p)]

    lines = [
        f"# é“¾ä¸Šé¡¹ç›®ç›‘æ§ - 48å°æ—¶æŠ¥å‘Š", "",
        f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        f"é¡¹ç›®æ€»æ•°: {len(active)} | AIæŒ–çŸ¿: {len(ai_list)} | å…¶ä»–: {len(normal_list)} | ç–‘ä¼¼å‡å¸‚å€¼: {len(fake_mc_list)}", "",
    ]
    if ai_list:
        lines += [f"## ğŸ¤– AI æŒ–çŸ¿é¡¹ç›® ({len(ai_list)})", ""]
        for i, p in enumerate(ai_list, 1):
            lines += [_fmt_project_md(p, i, _sc), "---", ""]
    if normal_list:
        lines += [f"## ğŸ“Š å…¶ä»–é¡¹ç›® ({len(normal_list)})", ""]
        for i, p in enumerate(normal_list, len(ai_list) + 1):
            lines += [_fmt_project_md(p, i, _sc), "---", ""]
    if fake_mc_list:
        lines += [f"## âš ï¸ ç–‘ä¼¼å‡å¸‚å€¼ ({len(fake_mc_list)})", ""]
        for i, p in enumerate(fake_mc_list, len(ai_list) + len(normal_list) + 1):
            lines += [_fmt_project_md(p, i, _sc), "---", ""]
    with open(REPORT_FILE, 'w') as f:
        f.write("\n".join(lines))
    log(f"[å½’æ¡£] 48hæŠ¥å‘Š: {len(active)} ä¸ªæ´»è·ƒé¡¹ç›®")

    # å½’æ¡£è¿‡æœŸé¡¹ç›®
    if expired:
        db = _load_archive_db()
        new_count = 0
        by_date = {}
        for p in expired:
            ts = p.get('open_timestamp', 0)
            date_str = datetime.fromtimestamp(ts).strftime('%Y-%m-%d') if ts else "unknown"
            by_date.setdefault(date_str, []).append(p)

        for date_str, dps in sorted(by_date.items()):
            existing_addrs = {p['address'] for p in db.get(date_str, [])}
            new_ps = [p for p in dps if p['address'] not in existing_addrs]
            if not new_ps:
                continue
            db.setdefault(date_str, []).extend(new_ps)
            new_count += len(new_ps)

            # å†™æ—¥æœŸå½’æ¡£æ–‡ä»¶
            all_day = db[date_str]
            all_day.sort(key=lambda x: (not x.get('is_ai_mining', False), -x.get('open_timestamp', 0)))
            day_ai = sum(1 for p in all_day if p.get('is_ai_mining'))
            day_sc = Counter(p['symbol'] for p in all_day)
            dl = [f"# é“¾ä¸Šé¡¹ç›®å½’æ¡£ - {date_str}", "",
                  f"é¡¹ç›®æ€»æ•°: {len(all_day)} | AIæŒ–çŸ¿: {day_ai}", ""]
            for i, p in enumerate(all_day, 1):
                dl += [_fmt_project_md(p, i, day_sc), "---", ""]
            with open(os.path.join(ARCHIVE_DIR, f"{date_str}.md"), 'w') as f:
                f.write("\n".join(dl))
            log(f"[å½’æ¡£] {date_str}: {len(all_day)} ä¸ªé¡¹ç›® (æ–°å¢ {len(new_ps)})")

        _save_archive_db(db)
        _update_index(db)
        log(f"[å½’æ¡£] å®Œæˆï¼Œæ–°å¢ {new_count} ä¸ªè¿‡æœŸé¡¹ç›®")


def cleanup_low_score_duplicates(state):
    """å®šæœŸæ¸…ç†ï¼š1.åŒåä»£å¸ä¸­è¯„åˆ†è¿‡ä½çš„ä»¿ç›˜(48hå) 2.æµåŠ¨æ€§æä½è¶…è¿‡24hçš„é¡¹ç›®"""
    notified_full = state.get('notified_full', {})
    notified_tokens = state.get('notified_tokens', {})

    removed = []

    # è§„åˆ™1: åŒåä»£å¸ä¸­ä½åˆ†ä»¿ç›˜48håæ¸…é™¤
    symbol_groups = defaultdict(list)
    for addr, p in list(notified_full.items()):
        sym = p.get('symbol', '').upper()
        if sym:
            symbol_groups[sym].append((addr, p))

    for sym, group in symbol_groups.items():
        if len(group) < 2:
            continue
        max_score = max(p.get('trust_score', 0) for _, p in group)
        if max_score == 0:
            continue
        for addr, p in group:
            score = p.get('trust_score', 0)
            rank = p.get('trust_rank', '')
            if 'ä»¿ç›˜' in rank and score <= max_score / 3 and p.get('age_hours', 0) > 48:
                removed.append((sym, addr[:10], score, 'ä½åˆ†ä»¿ç›˜'))
                notified_full.pop(addr, None)
                notified_tokens.pop(addr, None)

    # è§„åˆ™2: æµåŠ¨æ€§æä½(<$10K)ä¸”å¹´é¾„è¶…è¿‡24hçš„é¡¹ç›®æ¸…é™¤ï¼ˆAIæŒ–çŸ¿/æœ‰ç¤¾äº¤é“¾æ¥çš„è±å…ï¼‰
    for addr, p in list(notified_full.items()):
        liq = p.get('liquidity', 0) or 0
        age = p.get('age_hours', 0) or 0
        if liq < 10000 and age > 24:
            # è±å…ï¼šAIæŒ–çŸ¿é¡¹ç›®æˆ–æœ‰ç¤¾äº¤é“¾æ¥çš„é¡¹ç›®
            if p.get('is_ai_mining'):
                continue
            if p.get('twitter') or p.get('website'):
                continue
            removed.append((p.get('symbol', '?'), addr[:10], liq, 'æµåŠ¨æ€§æä½>24h'))
            notified_full.pop(addr, None)
            notified_tokens.pop(addr, None)

    if removed:
        log(f"[æ¸…ç†] ç§»é™¤ {len(removed)} ä¸ªé¡¹ç›®: {', '.join(f'{s}({a},{r})' for s,a,_,r in removed)}")
    return len(removed)


def fetch_honeypot_check(address):
    """é€šè¿‡ Honeypot.is API æ£€æµ‹èœœç½å’Œç¨ç‡"""
    try:
        resp = requests.get(
            f'https://api.honeypot.is/v2/IsHoneypot?address={address}&chainID=8453',
            timeout=10
        )
        d = resp.json()
        hp = d.get('honeypotResult', {})
        st = d.get('simulationResult', {})
        return {
            'is_honeypot': 1 if hp.get('isHoneypot') else 0,
            'buy_tax': str(st.get('buyTax', 0)),
            'sell_tax': str(st.get('sellTax', 0)),
        }
    except Exception as e:
        log(f"[honeypot] {address[:10]} error: {e}")
        return None


def fetch_token_latest(address):
    """é€šè¿‡ DexScreener API è·å–å•ä¸ªä»£å¸æœ€æ–°æ•°æ®"""
    try:
        resp = requests.get(
            f'https://api.dexscreener.com/latest/dex/tokens/{address}',
            timeout=10
        )
        data = resp.json()
        pairs = data.get('pairs', [])
        if not pairs:
            return None
        p = pairs[0]
        txns_1h = p.get('txns', {}).get('h1', {})
        info = p.get('info', {})
        return {
            'price': float(p.get('priceUsd', 0) or 0),
            'market_cap': float(p.get('marketCap', 0) or 0),
            'liquidity': float(p.get('liquidity', {}).get('usd', 0) or 0),
            'volume_1h': float(p.get('volume', {}).get('h1', 0) or 0),
            'buys': int(txns_1h.get('buys', 0)),
            'sells': int(txns_1h.get('sells', 0)),
            'price_change_1h': float(p.get('priceChange', {}).get('h1', 0) or 0),
        }
    except Exception as e:
        log(f"[fetch_token] {address[:10]} error: {e}")
        return None


def update_key_projects(state, merged):
    """æ¯è½®æ‰«ææ›´æ–°é‡ç‚¹è§‚å¯Ÿé¡¹ç›®ï¼ˆæœ‰ç¤¾äº¤é“¾æ¥æˆ–âœ…çœŸå“ï¼‰çš„å®æ—¶æ•°æ®"""
    merged_by_addr = {t['address']: t for t in merged}
    updated = 0
    api_fetched = 0
    now = time.time()
    update_keys = ['price', 'market_cap', 'liquidity', 'holders', 'price_change_1h',
                   'volume_1h', 'swaps', 'buys', 'sells', 'smart_buy_24h', 'smart_sell_24h',
                   'is_honeypot', 'buy_tax', 'sell_tax', 'renounced']

    for addr, old in state.get('notified_full', {}).items():
        # åªæ›´æ–°é‡ç‚¹é¡¹ç›®ï¼šæœ‰ç¤¾äº¤é“¾æ¥æˆ–âœ…çœŸå“
        is_key = bool(old.get('website')) or bool(old.get('twitter')) or 'çœŸå“' in old.get('trust_rank', '')
        if not is_key:
            continue

        new = merged_by_addr.get(addr)
        if not new:
            # å†·å´æœºåˆ¶ï¼š30åˆ†é’Ÿå†…æ›´æ–°è¿‡çš„è·³è¿‡ DexScreener æŸ¥è¯¢
            last_update = old.get('_last_api_update', 0)
            if now - last_update < 1800:
                continue
            # mergedï¼ˆGMGNï¼‰é‡Œæ²¡æœ‰ï¼Œç”¨ DexScreener å…œåº•
            new = fetch_token_latest(addr)
            if new:
                api_fetched += 1
                old['_last_api_update'] = now
                time.sleep(0.3)  # é˜²é™æµ

        if not new:
            continue
        for key in update_keys:
            if key in new and new[key] is not None:
                # buys/sells: ä¿ç•™è¾ƒå¤§å€¼ï¼ˆå†å²ç´¯è®¡ vs å½“å‰ï¼‰
                if key in ('buys', 'sells'):
                    old[key] = max(old.get(key, 0) or 0, new[key])
                else:
                    old[key] = new[key]
        # æ›´æ–°å¹´é¾„
        ots = old.get('open_timestamp', 0)
        if ots and ots > 1000000000:
            old['age_hours'] = round((time.time() - ots) / 3600, 1)
        updated += 1

    if updated:
        log(f"[æ›´æ–°] åˆ·æ–°äº† {updated} ä¸ªé‡ç‚¹é¡¹ç›®çš„å®æ—¶æ•°æ® (APIæŸ¥è¯¢: {api_fetched})")

    # èœœç½æ£€æµ‹ï¼šæ¯3è½®åšä¸€æ¬¡
    scan_count = state.get('_scan_count', 0)
    if scan_count % 3 == 0:
        hp_checked = 0
        for addr, old in state.get('notified_full', {}).items():
            is_key = bool(old.get('website')) or bool(old.get('twitter')) or 'çœŸå“' in old.get('trust_rank', '')
            if not is_key:
                continue
            # å·²ç¡®è®¤èœœç½ï¼Œæ°¸ä¸å†æ£€æµ‹
            if old.get('is_honeypot') == 1:
                continue
            # å·²ç¡®è®¤å®‰å…¨çš„ï¼Œ6å°æ—¶æ£€æµ‹ä¸€æ¬¡
            last_hp = old.get('_last_hp_check', 0)
            if old.get('is_honeypot') == 0 and last_hp > 0 and now - last_hp < 21600:
                continue
            # æœªæ£€æµ‹è¿‡çš„ï¼Œ1å°æ—¶å†·å´
            if last_hp > 0 and now - last_hp < 3600:
                continue
            if hp_checked >= 10:
                break
            hp = fetch_honeypot_check(addr)
            if hp:
                old['is_honeypot'] = hp['is_honeypot']
                old['buy_tax'] = hp['buy_tax']
                old['sell_tax'] = hp['sell_tax']
                old['_last_hp_check'] = now
                hp_checked += 1
                time.sleep(0.3)

        if hp_checked:
            log(f"[å®‰å…¨] èœœç½æ£€æµ‹äº† {hp_checked} ä¸ªé‡ç‚¹é¡¹ç›®")


def check_alerts(state, merged):
    """æ£€æµ‹AIæŒ–çŸ¿é¡¹ç›®æ¶¨å¹…å¼‚å¸¸ + æ”¶è—é¡¹ç›®é‡å¤§å˜åŒ–"""
    alerts = []
    merged_by_addr = {t['address']: t for t in merged}

    # åŠ è½½æ”¶è—åˆ—è¡¨
    favs = []
    if os.path.exists(FAV_FILE):
        with open(FAV_FILE) as f:
            favs = json.load(f)
    favs_set = set(favs)

    for addr, old in state.get('notified_full', {}).items():
        new = merged_by_addr.get(addr)
        if not new:
            continue

        # è¯»å–å½“å‰å’Œæ–°æ•°æ®ç”¨äºå‘Šè­¦åˆ¤æ–­ï¼ˆä¸åœ¨æ­¤å¤„æ›´æ–° stateï¼Œç”± update_key_projects ç»Ÿä¸€å¤„ç†ï¼‰
        old_price = float(old.get('price', 0) or 0)
        new_price = float(new.get('price', 0) or 0)
        old_liq = float(old.get('liquidity', 0) or 0)
        new_liq = float(new.get('liquidity', 0) or 0)
        old_mc = float(old.get('market_cap', 0) or 0)
        new_mc = float(new.get('market_cap', 0) or 0)

        try:
            chg_1h = float(new.get('price_change_1h', 0) or 0)
        except:
            chg_1h = 0

        # è§„åˆ™1: AIæŒ–çŸ¿é¡¹ç›®1hæ¶¨å¹…>500%
        if old.get('is_ai_mining') and chg_1h > 500:
            alerts.append({
                'type': 'surge',
                'symbol': old['symbol'],
                'address': addr,
                'change_1h': chg_1h,
                'market_cap': new_mc,
                'liquidity': new_liq,
                'holders': new.get('holders', 0),
            })

        # è§„åˆ™2: æ”¶è—é¡¹ç›®ä»·æ ¼å˜åŒ–>50% æˆ– æµåŠ¨æ€§å˜åŒ–>50%
        if addr in favs_set and old_price > 0 and new_price > 0:
            price_change = abs(new_price - old_price) / old_price
            liq_change = abs(new_liq - old_liq) / old_liq if old_liq > 0 else 0
            if price_change > 0.5 or liq_change > 0.5:
                alerts.append({
                    'type': 'fav_change',
                    'symbol': old['symbol'],
                    'address': addr,
                    'price_old': old_price,
                    'price_new': new_price,
                    'price_change_pct': price_change * 100,
                    'liq_old': old_liq,
                    'liq_new': new_liq,
                    'liq_change_pct': liq_change * 100,
                })

    if alerts:
        with open(ALERT_FILE, 'w') as f:
            json.dump({'time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'alerts': alerts}, f, ensure_ascii=False)
        log(f"ğŸš¨ ç”Ÿæˆ {len(alerts)} æ¡å‘Šè­¦")
        # å”¤é†’ AI
        try:
            text = f"é“¾ä¸Šå‘Šè­¦: {len(alerts)} æ¡"
            subprocess.Popen([
                'openclaw', 'system', 'event', '--text', text, '--mode', 'now'
            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        except:
            pass


def run():
    state = load_state()
    # ç¡®ä¿ notified_full å­—æ®µå­˜åœ¨
    if 'notified_full' not in state:
        state['notified_full'] = {}

    log(f"ğŸ” GMGN Monitor v2 started. Chain: {CHAIN}, Interval: {SCAN_INTERVAL}s")
    log(f"   æ•°æ®æº: GMGN-rank + GMGN-pairs + DexScreener")
    log(f"   è¿‡æ»¤: æµåŠ¨æ€§>=${MIN_LIQUIDITY} æŒæœ‰äºº>={MIN_HOLDERS} å¹´é¾„<={MAX_AGE_HOURS}h")
    log(f"   å½’æ¡£: {ARCHIVE_DIR}")

    while True:
        try:
            notified_set = set(state['notified_tokens'].keys())
            new_projects, merged = process_all(notified_set, state)

            if new_projects:
                log(f"âœ… å‘ç° {len(new_projects)} ä¸ªæ–°é¡¹ç›®!")
                ai_count = sum(1 for p in new_projects if p['is_ai_mining'])
                if ai_count:
                    log(f"ğŸ¤– å…¶ä¸­ {ai_count} ä¸ª AI æŒ–çŸ¿é¡¹ç›®!")

                # åŒåä»£å¸è¯„åˆ†
                try:
                    new_projects = detect_and_score_duplicates(new_projects, state)
                except Exception as e:
                    log(f"[è¯„åˆ†] Error: {e}")

                now = int(time.time())
                for p in new_projects:
                    state['notified_tokens'][p['address']] = now
                    state['notified_full'][p['address']] = p

                notify(new_projects)

                for p in new_projects[:15]:
                    tag = "ğŸ¤–" if p['is_ai_mining'] else "ğŸ“Š"
                    src = p.get('source', '?')[:3]
                    log(f"  {tag} {p['symbol']} | MC: ${p['market_cap']:,.0f} | "
                        f"Liq: ${p['liquidity']:,.0f} | Holders: {p['holders']} | "
                        f"Age: {p['age_hours']}h | Src: {src}")
            else:
                log("ğŸ“­ æœ¬è½®æ— æ–°é¡¹ç›®")

            # æ£€æµ‹å‘Šè­¦ï¼ˆæ¶¨å¹…å¼‚å¸¸ + æ”¶è—å˜åŒ–ï¼‰â€” å¿…é¡»åœ¨ update_key_projects ä¹‹å‰ï¼Œå¦åˆ™ old_price å·²è¢«æ›´æ–°
            try:
                check_alerts(state, merged)
            except Exception as e:
                log(f"[å‘Šè­¦] Error: {e}")

            # æ›´æ–°é‡ç‚¹é¡¹ç›®å®æ—¶æ•°æ®
            try:
                update_key_projects(state, merged)
            except Exception as e:
                log(f"[æ›´æ–°] Error: {e}")

            # æ¯è½®æ‰«æåæ‰§è¡Œå½’æ¡£
            try:
                archive_and_report(state)
            except Exception as e:
                log(f"[å½’æ¡£] Error: {e}")

            # æ¸…ç†ä½åˆ†ä»¿ç›˜
            try:
                cleanup_low_score_duplicates(state)
            except Exception as e:
                log(f"[æ¸…ç†] Error: {e}")

            state['last_scan'] = int(time.time())
            state['_scan_count'] = state.get('_scan_count', 0) + 1
            save_state(state)

        except Exception as e:
            log(f"âŒ Error: {e}")

        time.sleep(SCAN_INTERVAL)


if __name__ == '__main__':
    while True:
        try:
            run()
        except Exception as e:
            ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            print(f'[{ts}] ğŸ’€ run() crashed: {e}', flush=True)
            time.sleep(30)
        except KeyboardInterrupt:
            break
